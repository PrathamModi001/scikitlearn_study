{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf14bd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda79be3",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    "Always seperate out your test and training sets before doing any changes to the dataset \n",
    "like removing na or anything. Do filling/removing ONLY on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f61e90",
   "metadata": {},
   "source": [
    "# 1. Ready Your Data\n",
    "\n",
    "### Three main things: \n",
    "* Split the data into features and labels \n",
    "* Split data into training and test sets\n",
    "* Converting non numerical values to numerical (like Price)\n",
    "* Filling (imputing) or disregarding missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "46100115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak   \n",
       "0     63    1   3       145   233    1        0      150      0      2.3  \\\n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "0        0   0     1       1  \n",
       "1        0   0     2       1  \n",
       "2        2   0     2       1  \n",
       "3        2   0     2       1  \n",
       "4        2   0     2       1  \n",
       "..     ...  ..   ...     ...  \n",
       "298      1   0     3       0  \n",
       "299      1   0     3       0  \n",
       "300      1   2     3       0  \n",
       "301      1   1     3       0  \n",
       "302      1   1     2       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_disease = pd.read_csv(\"heart-disease.csv\")\n",
    "heart_disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fbb9d",
   "metadata": {},
   "source": [
    "## Split the Data into FEATURES and LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "31b382a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak   \n",
       "0     63    1   3       145   233    1        0      150      0      2.3  \\\n",
       "1     37    1   2       130   250    0        1      187      0      3.5   \n",
       "2     41    0   1       130   204    0        0      172      0      1.4   \n",
       "3     56    1   1       120   236    0        1      178      0      0.8   \n",
       "4     57    0   0       120   354    0        1      163      1      0.6   \n",
       "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "298   57    0   0       140   241    0        1      123      1      0.2   \n",
       "299   45    1   3       110   264    0        1      132      0      1.2   \n",
       "300   68    1   0       144   193    1        1      141      0      3.4   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "\n",
       "     slope  ca  thal  \n",
       "0        0   0     1  \n",
       "1        0   0     2  \n",
       "2        2   0     2  \n",
       "3        2   0     2  \n",
       "4        2   0     2  \n",
       "..     ...  ..   ...  \n",
       "298      1   0     3  \n",
       "299      1   0     3  \n",
       "300      1   2     3  \n",
       "301      1   1     3  \n",
       "302      1   1     2  \n",
       "\n",
       "[303 rows x 13 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every single column except target\n",
    "# Because target is what we have to predict \n",
    "# and other columns will be USED to predict target\n",
    "# so seperate them out\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ce57de69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "298    0\n",
       "299    0\n",
       "300    0\n",
       "301    0\n",
       "302    0\n",
       "Name: target, Length: 303, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = heart_disease[\"target\"]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a15638",
   "metadata": {},
   "source": [
    "## Split the data into TEST and TRAINING SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01036cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0061ba56",
   "metadata": {},
   "source": [
    "## Converting Non Numerical to Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3d7c4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales = pd.read_csv(\"https://raw.githubusercontent.com/mrdbourke/zero-to-mastery-ml/master/data/car-sales-extended.csv\")\n",
    "car_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fbb5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X/y\n",
    "X = car_sales.drop(\"Price\", axis=1)\n",
    "y = car_sales[\"Price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3a16dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories into Numbers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# KISKO CONVERT KARNA HAI ? Categories ko \n",
    "# Doors because it is categorical since 4 doors waali 856 cars hai, 5 doors waali 79, and 3: 65\n",
    "# We could categorise it !\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "\n",
    "# Instantiate OneHotEncoder (copy paste if you dont get it)\n",
    "one_hot = OneHotEncoder()\n",
    "\n",
    "# creating a transformer (copy paste, just change categorical features)\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                  one_hot,\n",
    "                                  categorical_features)],\n",
    "                                  remainder=\"passthrough\")\n",
    "\n",
    "# convert into numbers\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "95657f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(transformed_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f80dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOTHER WAY\n",
    "dummies = pd.get_dummies(car_sales[[\"Make\", \"Colour\", \"Doors\"]])\n",
    "dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d008cada",
   "metadata": {},
   "source": [
    "## Missing Values in Dataset\n",
    "* Fill them with some value (imputation)\n",
    "* Remove samples with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "decb9905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "car_sales_missing = pd.read_csv(\"car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.isna().sum()\n",
    "car_sales_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf400b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X/y\n",
    "\n",
    "X = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "95eccdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert data to numbers\n",
    "# Thus, categorise and convert them to numbers just like before\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                   one_hot,\n",
    "                                   categorical_features)],\n",
    "                                   remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "transformed_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c78f66",
   "metadata": {},
   "source": [
    "### Fill Missing Data using pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4ed913bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the \"Make\" column\n",
    "car_sales_missing[\"Make\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "# Fill the \"Colour\" column\n",
    "car_sales_missing[\"Colour\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "# Fill the \"Odometer (KM)\" column\n",
    "car_sales_missing[\"Odometer (KM)\"].fillna(car_sales_missing[\"Odometer (KM)\"].mean(), inplace=True)\n",
    "\n",
    "# Fill the \"Doors\" column\n",
    "car_sales_missing[\"Doors\"].fillna(4, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd9e4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum()\n",
    "# we can see all the nan features(X) have been removed\n",
    "# but we also have nan labels(y) , but since they are output,\n",
    "# if we dont have output how can we, at this point predict.\n",
    "\n",
    "# So we will Remove the column with no y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f7254",
   "metadata": {},
   "source": [
    "### Remove Empty Label list Row using pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "654c2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4769d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing.isna().sum() , len(car_sales_missing)\n",
    "# Now no missing values !\n",
    "# But we lost 50 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39c7d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2719aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert data to numbers\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\",\n",
    "                                   one_hot,\n",
    "                                   categorical_features)],\n",
    "                                   remainder=\"passthrough\")\n",
    "\n",
    "transformed_X = transformer.fit_transform(X)\n",
    "print(transformed_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41ea7d",
   "metadata": {},
   "source": [
    "## EXTRA: Check out Feature Scaling: \n",
    "In other words, making sure all of your numerical data is on the same scale.\n",
    "\n",
    "For example, say you were trying to predict the sale price of cars and the number of kilometres on their odometers varies from 6,000 to 345,000 but the median previous repair cost varies from 100 to 1,700. A machine learning algorithm may have trouble finding patterns in these wide-ranging variables.\n",
    "\n",
    "To fix this, there are two main types of feature scaling.\n",
    "\n",
    "Normalization (also called min-max scaling) - This rescales all the numerical values to between 0 and 1, with the lowest value being close to 0 and the highest previous value being close to 1. Scikit-Learn provides functionality for this in the MinMaxScalar class.\n",
    "\n",
    "Standardization - This subtracts the mean value from all of the features (so the resulting features have 0 mean). It then scales the features to unit variance (by dividing the feature by the standard deviation). Scikit-Learn provides functionality for this in the StandardScalar class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9b1afc",
   "metadata": {},
   "source": [
    "### Filling and transforming using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bad3b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_sales_missing = pd.read_csv(\"car-sales-extended-missing-data.csv\")\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63eb170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with no labels\n",
    "car_sales_missing.dropna(subset=[\"Price\"], inplace=True)\n",
    "car_sales_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af8e8336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into X & y\n",
    "X = car_sales_missing.drop(\"Price\", axis=1)\n",
    "y = car_sales_missing[\"Price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "24ba7251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2)\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8410285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with Scikit-Learn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Fill categorical values with 'missing' & numerical values with mean\n",
    "cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\") # Categorical Imputer\n",
    "door_imputer = SimpleImputer(strategy=\"constant\", fill_value=4)\n",
    "num_imputer = SimpleImputer(strategy=\"mean\") # Numerical Imputer\n",
    "\n",
    "# Define columns\n",
    "cat_features = [\"Make\", \"Colour\"]\n",
    "door_feature = [\"Doors\"]\n",
    "num_features = [\"Odometer (KM)\"]\n",
    "\n",
    "# Create an imputer (something that fills missing data)\n",
    "imputer = ColumnTransformer([\n",
    "    (\"cat_imputer\", cat_imputer, cat_features),\n",
    "    (\"door_imputer\", door_imputer, door_feature),\n",
    "    (\"num_imputer\", num_imputer, num_features)\n",
    "])\n",
    "\n",
    "# Fill train and test values separately\n",
    "filled_X_train = imputer.fit_transform(X_train) # since training set: using fit_transform\n",
    "filled_X_test = imputer.transform(X_test) # since test set: only transform\n",
    "\n",
    "# Check filled X_train\n",
    "filled_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d13a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our transformed data array's back into DataFrame's\n",
    "car_sales_filled_train = pd.DataFrame(filled_X_train, \n",
    "                                      columns=[\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])\n",
    "\n",
    "car_sales_filled_test = pd.DataFrame(filled_X_test, \n",
    "                                     columns=[\"Make\", \"Colour\", \"Doors\", \"Odometer (KM)\"])\n",
    "\n",
    "# Check missing data in training set\n",
    "car_sales_filled_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f359c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframes into numbers as before:\n",
    "categorical_features = [\"Make\", \"Colour\", \"Doors\"]\n",
    "one_hot = OneHotEncoder()\n",
    "transformer = ColumnTransformer([(\"one_hot\", \n",
    "                                 one_hot, \n",
    "                                 categorical_features)],\n",
    "                                 remainder=\"passthrough\")\n",
    "\n",
    "# Fill train and test values separately\n",
    "\n",
    "# fit_transform on training data\n",
    "transformed_X_train = transformer.fit_transform(car_sales_filled_train)\n",
    "# only transform on test data\n",
    "transformed_X_test = transformer.transform(car_sales_filled_test)\n",
    "\n",
    "# Check transformed and filled X_train\n",
    "transformed_X_train.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71027fd",
   "metadata": {},
   "source": [
    "# 2. Choosing an Estimator\n",
    "\n",
    "Some things to note:\n",
    "\n",
    "* Sklearn refers to machine learning models, algorithms as estimators.\n",
    "* Classification problem - predicting a category (heart disease or not)\n",
    "    * Sometimes you'll see clf (short for classifier) used as a classification estimator\n",
    "* Regression problem - predicting a number (selling price of a car)\n",
    "\n",
    "If you're working on a machine learning problem and looking to use Sklearn and not sure what model you should use, refer to the sklearn machine learning map: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c2bb5",
   "metadata": {},
   "source": [
    "1. If you have structured data: like Tables or Dataframes, prefer Ensemble Methods Like RandomForestClassifier/Regressor\n",
    "  \n",
    "2. If you have unstructured data, prefer Deep Learning or Transfer Learning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7456f6",
   "metadata": {},
   "source": [
    "### Regression Problem: Picking an estimator\n",
    "Let's use the California Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "83fe77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Getting it, since its inbuilt: \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9b6b73ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.DataFrame(housing[\"data\"], columns=housing[\"feature_names\"])\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d04e6f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we hqave to predict ? target: so make like an output column\n",
    "\n",
    "housing_df[\"target\"] = housing[\"target\"]\n",
    "housing_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd0987",
   "metadata": {},
   "source": [
    "So what do we have to do? Predict the value of target by using the other feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25299318",
   "metadata": {},
   "source": [
    "#### Step 1: Ready the Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "795622e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split in features and labels\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"] # which is: median houseprice in $100,000 (read docs)\n",
    "\n",
    "# Setup Random Seed: \n",
    "np.random.seed(42)\n",
    "\n",
    "# 2. Split the dataset into training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 4. Fill missing data: But no missing data is there: housing_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af93e3cc",
   "metadata": {},
   "source": [
    "#### Step 2: Choose an Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e02da0",
   "metadata": {},
   "source": [
    "##### Refer to the chart: \n",
    "Start -> Samples>50: Yes -> Categorical? No -> Predicting a quantity: Yes -> Samples>100k? No -> few features should be important: DONT KNOW: SO DO BOTH ALGOS  \n",
    "    \n",
    "RidgeRegression, if not work then Ensemble Regression  \n",
    "OR   \n",
    "Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "47790a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split in features and labels\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"] # which is: median houseprice in $100,000 (read docs)\n",
    "\n",
    "# Setup Random Seed: \n",
    "np.random.seed(42)\n",
    "\n",
    "# 2. Split the dataset into training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 4. Fill missing data: But no missing data is there: housing_df.isna().sum()\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# import estimator: RidgeRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Setup Random Seed: \n",
    "np.random.seed(42)\n",
    "\n",
    "# Instantiate and fit the model\n",
    "model = Ridge()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Check the score of the model (TEST SET)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3294232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "​\n",
    "import numpy as np# Standard Imports\n",
    "​\n",
    "import numpy as np# 1. Split in features and labels\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"] # which is: median houseprice in $100,000 (read docs)\n",
    "\n",
    "# Setup Random Seed: \n",
    "np.random.seed(42)\n",
    "\n",
    "# 2. Split the dataset into training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 4. Fill missing data: But no missing data is there: housing_df.isna().sum()\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# import Estimator: Lasso\n",
    "from sklearn import linear_model\n",
    "\n",
    "# Setup Random Seed: \n",
    "np.random.seed(42)\n",
    "\n",
    "model = linear_model.Lasso()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Check the score of the model (TEST SET)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c371c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Split in features and labels\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"] # which is: median houseprice in $100,000 (read docs)\n",
    "\n",
    "# Setup Random Seed: \n",
    "np.random.seed(42)\n",
    "\n",
    "# 2. Split the dataset into training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 4. Fill missing data: But no missing data is there: housing_df.isna().sum()\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# ENSEBLE Algorithm: SVR\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Setup Random Seed: \n",
    "np.random.seed(42)\n",
    "\n",
    "model = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Check the score of the model (TEST SET)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3c6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split in features and labels\n",
    "\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"] # which is: median houseprice in $100,000 (read docs)\n",
    "\n",
    "# Setup Random Seed: \n",
    "np.random.seed(42)\n",
    "\n",
    "# 2. Split the dataset into training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# 4. Fill missing data: But no missing data is there: housing_df.isna().sum()\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# ENSEMBLE Algorithm: RandomForestRegressor: Ensemble Regressor Algorithm\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Setup Random Seed: \n",
    "np.random.seed(42)\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)\n",
    "\n",
    "# This works the best !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd3faa",
   "metadata": {},
   "source": [
    "### Classification Problem: Choosing an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f416ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heart Disease was a classification problem \n",
    "heart_disease = pd.read_csv(\"heart-disease.csv\")\n",
    "heart_disease.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf61488",
   "metadata": {},
   "source": [
    "Refer the cheatsheet, it leads us to: LinearSVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a4993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup random Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data: \n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split into training and test datasets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# Import: \n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e6e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup random Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data: \n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split into training and test datasets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# Import: \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eaf31a",
   "metadata": {},
   "source": [
    "# 3. Fit the estimator to make predictions\n",
    "\n",
    "2 ways to make predictions: \n",
    "1. `predict()` \n",
    "2. `predict_proba()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6818b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup random Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Make the data: \n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split into training and test datasets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Import: \n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Instantiate The model\n",
    "model = LinearSVC()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test accuracy\n",
    "model.score(X_test, y_test)\n",
    "\n",
    "# Make predictions using the model\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637adb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will compare the predictions with the truth: y_test: \n",
    "np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions to truth labels to evaluate the model\n",
    "y_preds = model.predict(X_test)\n",
    "np.mean(y_preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c50281",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b603ee",
   "metadata": {},
   "source": [
    "`predict_proba()`   \n",
    "predict_proba() returns probabilities of a classification label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27bb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC ke liye alag hota hai idk why \n",
    "model._predict_proba_lr(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c788ee",
   "metadata": {},
   "source": [
    "it how certain your model is giving the predictions. we want it to be very confident.  \n",
    "0 aane ki 0.61 probability hai(61%)  \n",
    "1 aane ki 68%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b74c61",
   "metadata": {},
   "source": [
    "### `predict()` for our regression problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "46223f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf59baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create X y\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split into training and test datasets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test its accuracy\n",
    "model.score(X_test, y_test)\n",
    "\n",
    "# prediction\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2999963",
   "metadata": {},
   "source": [
    "# 4. Evaluating a model\n",
    "<br>\n",
    "\n",
    "### Classification Model:\n",
    "<br>\n",
    "        \n",
    "1. Estimator's built in: score() method  \n",
    "2. The scoring parameter  \n",
    "3. Problem specific evaluation metrics: Classification Report\n",
    "<br>\n",
    " <br>\n",
    "### Regression Model: \n",
    "<br>\n",
    "1. R^2 or coefficient of determination  <br>\n",
    "2. Mean Absolute Error (MAE)   <br>\n",
    "3. Mean Squared Error (MSE)   <br>\n",
    "4. Scoring Parameter here also  <br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9b5d2f",
   "metadata": {},
   "source": [
    "## Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f16901",
   "metadata": {},
   "source": [
    "### Score() Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55997be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a73a56",
   "metadata": {},
   "source": [
    "### scoring parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded33dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c7e850",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ba80e5",
   "metadata": {},
   "source": [
    "#### Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6239ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cross_val_acc = cross_val_score(model, X,y, scoring=None)\n",
    "cross_val_acc\n",
    "# if scoring=None, \n",
    "# esitmator's default scoring evaulation metric is used (accuracy for classification models)\n",
    "# Matlab for classification models: it uses scoring=\"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the mean of the 5 scores found\n",
    "np.random.seed(42)\n",
    "model_cross_val_score = np.mean(cross_val_acc)\n",
    "model_cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e1a44",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b7ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cross_val_prec = cross_val_score(model, X,y, scoring=\"precision\")\n",
    "cross_val_prec\n",
    "print(f\"The cross-validated precision is: {np.mean(cross_val_prec)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d8be6",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88abead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n",
    "np.random.seed(42)\n",
    "cv_recall = cross_val_score(model, X, y, cv=5, scoring=\"recall\")\n",
    "print(f\"The cross-validated recall is: {np.mean(cv_recall)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2f793",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "1. Accuracy, Precision, Recall, F1\n",
    "2. Area under ROC curve\n",
    "3. Confusion Matrix\n",
    "4. Classification Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b0b018",
   "metadata": {},
   "source": [
    "#### Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437eb049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create X & y\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Create model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_preds = model.predict(X_test)\n",
    "\n",
    "# Evaluate model using evaluation functions\n",
    "print(\"Classifier metrics on the test set\")\n",
    "print(f\"Accurracy: {accuracy_score(y_test, y_preds)*100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(y_test, y_preds)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_preds)}\")\n",
    "print(f\"F1: {f1_score(y_test, y_preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889d04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Heart Disease Classifier has a Cross-Validated Accuracy of {model_cross_val_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f16f47",
   "metadata": {},
   "source": [
    "#### Area under the Reciever Operating Characteristic Curve (AUC/ROC Curve)\n",
    "#### BINARY CLASSIFICATION PROBLEMS ONLY !\n",
    "\n",
    "\n",
    "ROC curves are a comparison of a model's true postive rate (tpr) versus a models false positive rate (fpr).  \n",
    "\n",
    "* True positive = model predicts 1 when truth is 1  \n",
    "* False positive = model predicts 1 when truth is 0 \n",
    "* True negative = model predicts 0 when truth is 0  \n",
    "* False negative = model predicts 0 when truth is 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e7191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = heart_disease.drop(\"target\", axis=1)\n",
    "y = heart_disease[\"target\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train);\n",
    "\n",
    "y_probs = model.predict_proba(X_test)\n",
    "y_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c776980",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_positive = y_probs[:, 1:]\n",
    "y_probs_positive[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d791cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fpr, tpr, thresholds: \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs_positive)\n",
    "\n",
    "# Check positive rates: \n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ec6945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a function to plot roc_curve so it makes more sense\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a Figure and Axes object\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot roc curve\n",
    "ax.plot(fpr, tpr, color=\"orange\", label=\"ROC\")\n",
    "\n",
    "# Plot line with no predictive power (baseline)\n",
    "ax.plot([0, 1], [0, 1], color=\"darkblue\", linestyle=\"--\", label=\"Guessing\")\n",
    "\n",
    "# Customize the plot\n",
    "ax.set(xlabel=\"False positive rate (fpr)\", \n",
    "       ylabel=\"True positive rate (tpr)\", \n",
    "       title=\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72db918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the auc score: \n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_test, y_probs_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7962cc",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "  \n",
    "A confusion matrix is a quick way to compare the labels a model predicts and the actual labels it was supposed to predict. In essence, giving you an idea of where the model is getting confused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "confusion_matrix(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3561b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix: \n",
    "pd.crosstab(y_test, \n",
    "            y_preds, \n",
    "            rownames=[\"Actual Label\"], \n",
    "            colnames=[\"Predicted Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f69936",
   "metadata": {},
   "source": [
    "So we can see that our model is predicting 0 as 0 (correct) 24 times BUT it is predicting 1 when actual value was 0 (wrong)  \n",
    "Diagonals 5,4 are times our model was wrong (was getting confused)  \n",
    "<br>\n",
    "    \n",
    "SO 24,28 are giving us true negatives and true positives,  \n",
    "WHILE 5,4 are giving us false negatives and false positives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb224e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another Visualisation of Confusion Matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(estimator=model, X=X, y=y);\n",
    "\n",
    "# OR YOU CAN DO: \n",
    "\n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6bf314",
   "metadata": {},
   "source": [
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7da3f",
   "metadata": {},
   "source": [
    "<img src=\"./Images/classification-report.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff833df",
   "metadata": {},
   "source": [
    "## Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962d550",
   "metadata": {},
   "source": [
    "### R^2 or coefficient of determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82683dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create X y\n",
    "X = housing_df.drop(\"target\", axis=1)\n",
    "y = housing_df[\"target\"]\n",
    "\n",
    "# Split into training and test datasets:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# SCORE USUS R^2 ITSELF !\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9e35a",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (MAE)\n",
    "<br>\n",
    "MAE is the average of the absolute differences between predictions and actual values.\n",
    "\n",
    "It gives you an idea of how wrong your models predictions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0fe2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "np.random.seed(42)\n",
    "\n",
    "y_preds = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_preds)\n",
    "mae\n",
    "\n",
    "# This means OUR MODELS values are +/-0.32 from the actual value. It is OFF by 0.32\n",
    "# You can say its the mean of the absolute differences btw actual and predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf942d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"actual values\": y_test,\n",
    "                        \"predicted values\": y_preds})\n",
    "df[\"differences\"] = df[\"predicted values\"] - df[\"actual values\"]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be858dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE using formulas and differences\n",
    "np.random.seed(42)\n",
    "np.abs(df[\"differences\"]).mean()\n",
    "# Absolute of the differences: and its mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47a654",
   "metadata": {},
   "source": [
    "### Mean Squared Error: MSE\n",
    "<br>\n",
    "MSE is the mean of the square of the errors between actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "y_preds = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_preds)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c094af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"squared_differences\"] = np.square(df[\"differences\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55200a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE by hand/formula\n",
    "np.random.seed(42)\n",
    "squared = np.square(df[\"differences\"])\n",
    "squared.mean()\n",
    "\n",
    "# This means OUR MODELS values are +/-0.25 from the actual value. It is OFF by 0.25\n",
    "# You can say its the mean of the absolute differences btw actual and predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff19da58",
   "metadata": {},
   "source": [
    "### Scoring Paramter for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8960330",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_r2 = cross_val_score(model, X, y, cv=3, scoring=None)\n",
    "np.mean(cv_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared error\n",
    "np.random.seed(42)\n",
    "cv_mse = cross_val_score(model, X, y, cv=5, scoring=\"neg_mean_squared_error\")\n",
    "np.mean(cv_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720a578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean absolute error\n",
    "cv_mae = cross_val_score(model, X, y, cv=5, scoring=\"neg_mean_absolute_error\")\n",
    "np.mean(cv_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d24d1",
   "metadata": {},
   "source": [
    "<img src=\"./Images/regression-evaluation.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b6753",
   "metadata": {},
   "source": [
    "# Choosing Of all: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09f35a",
   "metadata": {},
   "source": [
    "<img src=\"./Images/Evaluation/1.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d1c479",
   "metadata": {},
   "source": [
    "<img src=\"./Images/Evaluation/2.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcae0e4",
   "metadata": {},
   "source": [
    "<img src=\"./Images/Evaluation/3.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4640bd3",
   "metadata": {},
   "source": [
    "<img src=\"./Images/Evaluation/4.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51da2e",
   "metadata": {},
   "source": [
    "# 5. Improving a model \n",
    "\n",
    "First predictions = baseline predictions.\n",
    "First model = baseline model.\n",
    "\n",
    "From a data perspective:\n",
    "* Could we collect more data? (generally, the more data, the better) \n",
    "* Could we improve our data? \n",
    "\n",
    "From a model perspective:\n",
    "* Is there a better model we could use?\n",
    "* Could we improve the current model? \n",
    "\n",
    "Hyperparameters vs. Parameters\n",
    "* Parameters = model find these patterns in data\n",
    "* Hyperparameters = settings on a model you can adjust to (potentially) improve its ability to find patterns\n",
    "\n",
    "Three ways to adjust hyperparameters:\n",
    "1. By hand\n",
    "2. Randomly with RandomSearchCV\n",
    "3. Exhaustively with GridSearchCV  \n",
    " <br>  \n",
    " #### When comparing models, you should be careful to make sure they're compared on the same splits of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b375d",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters BY HAND\n",
    "<br>\n",
    "\n",
    "Let's make 3 sets, training, validation and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47ac319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.get_params()\n",
    "\n",
    "# These are yhe paramters we can change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419e834",
   "metadata": {},
   "source": [
    " We're going to try and adjust:\n",
    "\n",
    "* `max_depth`\n",
    "* `max_features`\n",
    "* `min_samples_leaf`\n",
    "* `min_samples_split`\n",
    "* `n_estimators`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bdc786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    Performs evaluation comparison on y_true labels vs. y_pred labels\n",
    "    on a classification.\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_preds)\n",
    "    precision = precision_score(y_true, y_preds)\n",
    "    recall = recall_score(y_true, y_preds)\n",
    "    f1 = f1_score(y_true, y_preds)\n",
    "    # return a dictionary with the  evaluations\n",
    "    metric_dict = {\"accuracy\": round(accuracy, 2),\n",
    "                   \"precision\": round(precision, 2),\n",
    "                   \"recall\": round(recall, 2),\n",
    "                   \"f1\": round(f1, 2)}\n",
    "    print(f\"Acc: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 score: {f1:.2f}\")\n",
    "    \n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4815f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Shuffle the data \n",
    "heart_disease_shuffled = heart_disease.sample(frac=1) # shuffle 100% of the data\n",
    "\n",
    "# Split into X and y\n",
    "X = heart_disease_shuffled.drop(\"target\", axis=1)\n",
    "y = heart_disease_shuffled[\"target\"]\n",
    "\n",
    "# Split into TRAIN TEST and VALIDATIONS sets:\n",
    "# 70% on training data, 15% on validation and rest 15% on test\n",
    "\n",
    "train_split = round(0.7 * len(heart_disease_shuffled))\n",
    "valid_split = round(train_split + 0.15 * len(heart_disease_shuffled)) # NEXT 15% of data\n",
    "\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_valid, y_valid = X[train_split:valid_split], y[train_split:valid_split]\n",
    "X_test, y_test = X[valid_split:], y[:valid_split]\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the valid set\n",
    "y_preds = model.predict(X_valid)\n",
    "\n",
    "# Evaluate on valid set\n",
    "baseline_metrics = evaluate_preds(y_valid, y_preds)\n",
    "baseline_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20ee500",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Create a second classifier with different hyperparameters\n",
    "model_2 = RandomForestClassifier(max_depth=10)\n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "y_preds_2 = model_2.predict(X_valid)\n",
    "\n",
    "model_2_metrics = evaluate_preds(y_valid, y_preds_2)\n",
    "\n",
    "# Very difficult to change these by hand so will use inbuilt scikit learn function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5dacfb",
   "metadata": {},
   "source": [
    "## RandomisedSearchCV: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6825c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "grid = {\"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n",
    "        \"max_depth\": [None, 5, 10, 20, 30],\n",
    "#         \"max_features\": [\"auto\", \"sqrt\"],\n",
    "        \"min_samples_split\": [2, 4, 6],\n",
    "        \"min_samples_leaf\": [1, 2, 4]}\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X & y\n",
    "X = heart_disease_shuffled.drop(\"target\", axis=1)\n",
    "y = heart_disease_shuffled[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "rs_clf = RandomizedSearchCV(estimator=clf,\n",
    "                            param_distributions=grid, \n",
    "                            n_iter=20, # number of models to try\n",
    "                            cv=5,\n",
    "                            verbose=2)\n",
    "\n",
    "# Fit the RandomizedSearchCV version of clf\n",
    "rs_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1031aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9243fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with the best found model\n",
    "rs_clf_preds = rs_clf.predict(X_test)\n",
    "\n",
    "# Evaluating this model with the function we created: \n",
    "rs_metrics = evaluate_preds(y_test, rs_clf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cbd528",
   "metadata": {},
   "source": [
    " ## GridSearchCV: Hyperparameter Training \n",
    " \n",
    " RandomSearchCV is like brute force method\n",
    " because:   \n",
    " 6 * 5 * 3 * 3 * 5(cross valid=5) => 1350  \n",
    " Thats a LOTT for your computer and consider a large dataset !\n",
    " \n",
    " , but gridSearch will reduce these numbers drastically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid\n",
    "# we must reduce the parameters we are passing here: but how to choose which to keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a70168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new grid based on the results of randomsearch: \n",
    "grid_2 = {'n_estimators': [100, 200, 500],\n",
    "          'max_depth': [None],\n",
    "          'max_features': ['sqrt'],\n",
    "          'min_samples_split': [6],\n",
    "          'min_samples_leaf': [1, 2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f187648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Split into X & y\n",
    "X = heart_disease_shuffled.drop(\"target\", axis=1)\n",
    "y = heart_disease_shuffled[\"target\"]\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate RandomForestClassifier\n",
    "model = RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs_model = GridSearchCV(estimator=model,\n",
    "                      param_grid=grid_2, \n",
    "                      cv=5,\n",
    "                      verbose=2)\n",
    "\n",
    "# Fit the GridSearchCV version of clf\n",
    "gs_model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a70d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6167ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_y_preds = gs_model.predict(X_test)\n",
    "\n",
    "# evaluate the predictions\n",
    "gs_metrics = evaluate_preds(y_test, gs_y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca39d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metrics = pd.DataFrame({\"baseline\": baseline_metrics,\n",
    "                                \"model_2\": model_2_metrics,\n",
    "                                \"random search\": rs_metrics,\n",
    "                                \"grid search\": gs_metrics})\n",
    "\n",
    "compare_metrics.plot.bar(figsize=(10, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd62ad",
   "metadata": {},
   "source": [
    "# 6. Saving and loading trained ML models\n",
    "\n",
    "Two ways to save and load machine learning models:\n",
    "1. With Python's `pickle` module\n",
    "2. With the `joblib` module\n",
    "\n",
    "**Pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ecd8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save an extisting model to file\n",
    "pickle.dump(gs_model, open(\"gs.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da427e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model\n",
    "loaded_pickle_model = pickle.load(open(\"gs.pkl\", \"rb\")) # read bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6fd04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some predictions\n",
    "pickle_y_preds = loaded_pickle_model.predict(X_test)\n",
    "evaluate_preds(y_test, pickle_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4315d92",
   "metadata": {},
   "source": [
    "**Joblib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d2ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "# Save model to file\n",
    "dump(gs_model, filename=\"gs_joblib.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f6f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a saved joblib model\n",
    "loaded_joblib_model = load(filename=\"gs_joblib.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf04688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make and evaluate joblib predictions\n",
    "joblib_y_preds = loaded_joblib_model.predict(X_test)\n",
    "evaluate_preds(y_test, joblib_y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a36925",
   "metadata": {},
   "source": [
    "# 7. Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f863b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"car-sales-extended-missing-data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa810177",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c088a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting data ready: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Setup random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import data: \n",
    "data = pd.read_csv(\"car-sales-extended-missing-data.csv\")\n",
    "\n",
    "# Dropping target wale missing values\n",
    "data.dropna(subset=[\"Price\"], inplace=True)\n",
    "\n",
    "# Define different features and transformer pipeline\n",
    "categorical_features = [\"Make\", \"Colour\"]\n",
    "# Steps matlab pehle wo impute karega THEN wo onehot encode karega\n",
    "# ek hi variable se dono karna hai: thus pipeline use\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "door_feature = [\"Doors\"]\n",
    "door_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=4))\n",
    "])\n",
    "\n",
    "numeric_features = [\"Odometer (KM)\"]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\"))\n",
    "])\n",
    "\n",
    "# Setup preprocessing steps (fill missing values, then convert to numbers)\n",
    "# Applying the transformer to the different features in one go: thus a pipeline used\n",
    "preprocessor = ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        (\"cat\", categorical_transformer, categorical_features),\n",
    "                        (\"door\", door_transformer, door_feature),\n",
    "                        (\"num\", numeric_transformer, numeric_features)\n",
    "                    ])\n",
    "\n",
    "# Creating a preprocessing and modelling pipeline\n",
    "model = Pipeline(steps=[(\"preprocessor\", preprocessor),\n",
    "                        (\"model\", RandomForestRegressor())])\n",
    "\n",
    "# Split data\n",
    "X = data.drop(\"Price\", axis=1)\n",
    "y = data[\"Price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Fit and score the model\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e6545",
   "metadata": {},
   "source": [
    "It's also possible to use `GridSearchCV` or `RandomizedSesrchCV` with our `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e6608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV with our regression Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_grid = {\n",
    "# preprocessor keyword used in model\n",
    "# num is the transformation of numeric_features using numeric_transformer (inside proprocessor)\n",
    "# the attriibute of num_feature: imputer\n",
    "# the strategy used will be : mean or median\n",
    "# SO IN TOTAL ITS SAYING: \n",
    "# in preprocessor, num step, the attriibute of num_feature: imputer, we use stategy: mean/median\n",
    "    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n",
    "    \"model__n_estimators\": [100, 1000],\n",
    "    \"model__max_depth\": [None, 5],\n",
    "    \"model__min_samples_split\": [2, 4]    \n",
    "}\n",
    "\n",
    "gs_model = GridSearchCV(model, pipe_grid, cv=5, verbose=2)\n",
    "gs_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc491a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2233a67b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
